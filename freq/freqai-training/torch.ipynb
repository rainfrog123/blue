{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features shape: (17721, 1207)\n",
      "\n",
      "Randomly selected 20 features:\n",
      "['%-plus_di-period_100_ETH/USDTUSDT_1m', '%-ad-period_100_shift-1_ETH/USDTUSDT_1m', '%-bb_upper-period_14_shift-3_ETH/USDTUSDT_15m', '%-obv-period_100_ETH/USDTUSDT_1m', '%-rsi-period_28_ETH/USDTUSDT_15m', '%-lower_shadow-period_100_ETH/USDTUSDT_15m', '%-bb_middle-period_14_shift-2_ETH/USDTUSDT_1m', '%-rsi-period_28_ETH/USDTUSDT_1m', '%-obv-period_14_shift-3_ETH/USDTUSDT_1m', '%-tema-period_14_shift-2_ETH/USDTUSDT_15m', '%-plus_di-period_14_shift-3_ETH/USDTUSDT_1m', '%-willr-period_100_shift-3_ETH/USDTUSDT_1m', '%-cci-period_14_shift-1_ETH/USDTUSDT_1m', '%-wma-period_100_shift-2_ETH/USDTUSDT_1m', '%-ohlc4-period_100_shift-2_ETH/USDTUSDT_15m', '%-stochf_k-period_28_ETH/USDTUSDT_1m', '%-ohlc4-period_28_shift-3_ETH/USDTUSDT_15m', '%-trix-period_28_shift-1_ETH/USDTUSDT_1m', '%-volume_change-period_100_shift-1_ETH/USDTUSDT_1m', '%-tema-period_28_shift-2_ETH/USDTUSDT_15m']\n",
      "\n",
      "Dataset splits:\n",
      "Training set size: 14176\n",
      "Validation set size: 3545\n",
      "Input dimensions: 20\n",
      "Output dimensions: 2\n",
      "\n",
      "Batch test results:\n",
      "Features shape: torch.Size([64, 20])\n",
      "Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, features: torch.Tensor, labels: torch.Tensor):\n",
    "        \"\"\"Initialize dataset with pre-processed tensors for better efficiency.\"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class CryptoDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        directory_path: str, \n",
    "        batch_size: int = 128, \n",
    "        num_workers: int = 4, \n",
    "        train_split: float = 0.8, \n",
    "        n_features: int = 20,\n",
    "        random_seed: int = 42\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.directory_path = Path(directory_path)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_split = train_split\n",
    "        self.n_features = n_features\n",
    "        self.random_seed = random_seed\n",
    "        self.scaler = StandardScaler()\n",
    "        self.selected_features = None\n",
    "        self.input_dim = None\n",
    "        self.output_dim = None\n",
    "        \n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Verify data files exist.\"\"\"\n",
    "        features_path = self.directory_path / 'ETH/USDT:USDT_raw_features_20250206_184554.parquet'\n",
    "        labels_path = self.directory_path / 'ETH/USDT:USDT_raw_labels_20250206_184554.parquet'\n",
    "        \n",
    "        if not (features_path.exists() and labels_path.exists()):\n",
    "            raise FileNotFoundError(f\"Data files not found in {self.directory_path}/ETH/\")\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Setup data with efficient preprocessing.\"\"\"\n",
    "        # Load data\n",
    "        features = pd.read_parquet(self.directory_path / 'ETH/USDT:USDT_raw_features_20250206_184554.parquet')\n",
    "        labels = pd.read_parquet(self.directory_path / 'ETH/USDT:USDT_raw_labels_20250206_184554.parquet')\n",
    "        \n",
    "        # Print initial info\n",
    "        print(f\"Original features shape: {features.shape}\")\n",
    "        \n",
    "        # Select features (use numpy for efficiency)\n",
    "        np.random.seed(self.random_seed)\n",
    "        self.selected_features = np.random.choice(features.columns, size=self.n_features, replace=False)\n",
    "        features = features[self.selected_features]\n",
    "        \n",
    "        print(f\"\\nRandomly selected {self.n_features} features:\")\n",
    "        print(self.selected_features.tolist())\n",
    "        \n",
    "        # Efficient preprocessing\n",
    "        features = features.fillna(0).values  # Convert to numpy array\n",
    "        features = self.scaler.fit_transform(features)  # Scale features\n",
    "        labels = LabelEncoder().fit_transform(labels['&-target'].values)\n",
    "        \n",
    "        # Convert to tensors once\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = CryptoDataset(features_tensor, labels_tensor)\n",
    "        \n",
    "        # Split dataset\n",
    "        train_size = int(self.train_split * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        \n",
    "        self.train_dataset, self.val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(self.random_seed)\n",
    "        )\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.input_dim = self.n_features\n",
    "        self.output_dim = len(np.unique(labels))\n",
    "        \n",
    "        # Print setup info\n",
    "        print(f\"\\nDataset splits:\")\n",
    "        print(f\"Training set size: {train_size}\")\n",
    "        print(f\"Validation set size: {val_size}\")\n",
    "        print(f\"Input dimensions: {self.input_dim}\")\n",
    "        print(f\"Output dimensions: {self.output_dim}\")\n",
    "        \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers, \n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            persistent_workers=True if self.num_workers > 0 else False\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers, \n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            persistent_workers=True if self.num_workers > 0 else False\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with deterministic behavior\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    # Initialize data module\n",
    "    data_module = CryptoDataModule(\n",
    "        directory_path='/allah/data/parquet',\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        n_features=20\n",
    "    )\n",
    "    \n",
    "    # Setup and test\n",
    "    data_module.setup()\n",
    "    \n",
    "    # Test batch loading\n",
    "    train_loader = data_module.train_dataloader()\n",
    "    features, labels = next(iter(train_loader))\n",
    "    \n",
    "    print(\"\\nBatch test results:\")\n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoPricePredictor(pl.LightningModule):\n",
    "    def __init__(self, input_dim=20, hidden_dims=[512, 256, 128], dropout_rate=0.3, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Initialize class weights\n",
    "        self.register_buffer('class_weights', torch.tensor([1.0, 1.0]))\n",
    "        \n",
    "        # Model architecture\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        \n",
    "        for i in range(len(dims)-1):\n",
    "            layers.extend([\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "        \n",
    "        layers.append(nn.Linear(dims[-1], 2))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y, weight=self.class_weights)\n",
    "        \n",
    "        # Log training metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y, weight=self.class_weights)\n",
    "        \n",
    "        # Log validation loss\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features shape: (17721, 1207)\n",
      "\n",
      "Randomly selected 20 features:\n",
      "['%-plus_di-period_100_ETH/USDTUSDT_1m', '%-ad-period_100_shift-1_ETH/USDTUSDT_1m', '%-bb_upper-period_14_shift-3_ETH/USDTUSDT_15m', '%-obv-period_100_ETH/USDTUSDT_1m', '%-rsi-period_28_ETH/USDTUSDT_15m', '%-lower_shadow-period_100_ETH/USDTUSDT_15m', '%-bb_middle-period_14_shift-2_ETH/USDTUSDT_1m', '%-rsi-period_28_ETH/USDTUSDT_1m', '%-obv-period_14_shift-3_ETH/USDTUSDT_1m', '%-tema-period_14_shift-2_ETH/USDTUSDT_15m', '%-plus_di-period_14_shift-3_ETH/USDTUSDT_1m', '%-willr-period_100_shift-3_ETH/USDTUSDT_1m', '%-cci-period_14_shift-1_ETH/USDTUSDT_1m', '%-wma-period_100_shift-2_ETH/USDTUSDT_1m', '%-ohlc4-period_100_shift-2_ETH/USDTUSDT_15m', '%-stochf_k-period_28_ETH/USDTUSDT_1m', '%-ohlc4-period_28_shift-3_ETH/USDTUSDT_15m', '%-trix-period_28_shift-1_ETH/USDTUSDT_1m', '%-volume_change-period_100_shift-1_ETH/USDTUSDT_1m', '%-tema-period_28_shift-2_ETH/USDTUSDT_15m']\n",
      "\n",
      "Dataset splits:\n",
      "Training set size: 14176\n",
      "Validation set size: 3545\n",
      "Input dimensions: 20\n",
      "Output dimensions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/allah/freqtrade/.venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /allah/data/parquet/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 177 K  | train\n",
      "---------------------------------------------\n",
      "177 K     Trainable params\n",
      "0         Non-trainable params\n",
      "177 K     Total params\n",
      "0.708     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features shape: (17721, 1207)\n",
      "\n",
      "Randomly selected 20 features:\n",
      "['%-plus_di-period_100_ETH/USDTUSDT_1m', '%-ad-period_100_shift-1_ETH/USDTUSDT_1m', '%-bb_upper-period_14_shift-3_ETH/USDTUSDT_15m', '%-obv-period_100_ETH/USDTUSDT_1m', '%-rsi-period_28_ETH/USDTUSDT_15m', '%-lower_shadow-period_100_ETH/USDTUSDT_15m', '%-bb_middle-period_14_shift-2_ETH/USDTUSDT_1m', '%-rsi-period_28_ETH/USDTUSDT_1m', '%-obv-period_14_shift-3_ETH/USDTUSDT_1m', '%-tema-period_14_shift-2_ETH/USDTUSDT_15m', '%-plus_di-period_14_shift-3_ETH/USDTUSDT_1m', '%-willr-period_100_shift-3_ETH/USDTUSDT_1m', '%-cci-period_14_shift-1_ETH/USDTUSDT_1m', '%-wma-period_100_shift-2_ETH/USDTUSDT_1m', '%-ohlc4-period_100_shift-2_ETH/USDTUSDT_15m', '%-stochf_k-period_28_ETH/USDTUSDT_1m', '%-ohlc4-period_28_shift-3_ETH/USDTUSDT_15m', '%-trix-period_28_shift-1_ETH/USDTUSDT_1m', '%-volume_change-period_100_shift-1_ETH/USDTUSDT_1m', '%-tema-period_28_shift-2_ETH/USDTUSDT_15m']\n",
      "\n",
      "Dataset splits:\n",
      "Training set size: 14176\n",
      "Validation set size: 3545\n",
      "Input dimensions: 20\n",
      "Output dimensions: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be872c029e954fc7942644d30579e13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506f12bf05784e5db9ac495a24ad5bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2ce4678ab144709403ffcf2587457b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.652\n",
      "Epoch 0, global step 221: 'val_loss' reached 0.65173 (best 0.65173), saving model to '/allah/data/parquet/checkpoints/model-epoch=00-val_loss=0.652.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f68ff4144c4d87a68d2b77f7b6950f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 442: 'val_loss' reached 0.65831 (best 0.65173), saving model to '/allah/data/parquet/checkpoints/model-epoch=01-val_loss=0.658.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff5a5d8e62c4613b1de8e5e8ad6ec82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 663: 'val_loss' reached 0.65307 (best 0.65173), saving model to '/allah/data/parquet/checkpoints/model-epoch=02-val_loss=0.653.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fe1a9de8a44909b4b4fe94644d8129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 884: 'val_loss' reached 0.65565 (best 0.65173), saving model to '/allah/data/parquet/checkpoints/model-epoch=03-val_loss=0.656.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673d9f4ed6954586b56afa5e36ddf406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1105: 'val_loss' reached 0.65138 (best 0.65138), saving model to '/allah/data/parquet/checkpoints/model-epoch=04-val_loss=0.651.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7883203ccf224469b1e086ae7eae6801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1326: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba87badc197445b9bc61b362fafce1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1547: 'val_loss' reached 0.65203 (best 0.65138), saving model to '/allah/data/parquet/checkpoints/model-epoch=06-val_loss=0.652.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60482509294748ca883cb2134351a2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1768: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08bc13161f0443eacf2ede0c429ee76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1989: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780bbadacc364200accac134b296ce14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 2210: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88540f6175474aa9953e17e0f7084992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 2431: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e08d94927c4b77bea2b83f3e587d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 2652: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0825e0f5b64a4c945890a49acb6433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 2873: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82511f11ee8430f8e9fef568f8d460c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 3094: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d845e54374039a0ee2d401c09fe21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 3315: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d732956252044838ba6c962db1de7364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 15 records. Best score: 0.652. Signaling Trainer to stop.\n",
      "Epoch 15, global step 3536: 'val_loss' reached 0.65131 (best 0.65131), saving model to '/allah/data/parquet/checkpoints/model-epoch=15-val_loss=0.651.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss: 0.6513\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Directory setup\n",
    "    directory_path = '/allah/data/parquet'\n",
    "    \n",
    "    # Initialize data module\n",
    "    data_module = CryptoDataModule(\n",
    "        directory_path=directory_path,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        n_features=20\n",
    "    )\n",
    "    data_module.setup()\n",
    "\n",
    "    # Model initialization\n",
    "    model = CryptoPricePredictor(input_dim=data_module.input_dim)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        patience=15,\n",
    "        min_delta=0.001,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='checkpoints',\n",
    "        filename='model-{epoch:02d}-{val_loss:.3f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=100,\n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        logger=True,  # Use default logger\n",
    "        accelerator='auto',\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        log_every_n_steps=1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    try:\n",
    "        trainer.fit(model, datamodule=data_module)\n",
    "        print(f\"Best validation loss: {checkpoint_callback.best_model_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cell\n",
    "import optuna\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "class OptunaObjective:\n",
    "    def __init__(self, data_module: CryptoDataModule):\n",
    "        self.data_module = data_module\n",
    "        \n",
    "    def __call__(self, trial: optuna.Trial) -> float:\n",
    "        # Define hyperparameters to optimize\n",
    "        n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "        hidden_dims = []\n",
    "        for i in range(n_layers):\n",
    "            hidden_dims.append(trial.suggest_int(f'hidden_dim_{i}', 64, 512, step=64))\n",
    "        \n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'hidden_dims': hidden_dims,  # Store the full list\n",
    "            'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "        }\n",
    "        \n",
    "        # Update data module batch size\n",
    "        self.data_module.batch_size = params['batch_size']\n",
    "        self.data_module.setup()\n",
    "        \n",
    "        # Initialize model with trial params\n",
    "        model = CryptoPricePredictor(\n",
    "            input_dim=self.data_module.input_dim,\n",
    "            hidden_dims=params['hidden_dims'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            learning_rate=params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        # Training callbacks\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                monitor=\"val_win_precision\",\n",
    "                mode=\"max\",\n",
    "                save_top_k=1,\n",
    "                save_weights_only=True,\n",
    "                verbose=False\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_win_precision\",\n",
    "                mode=\"max\",\n",
    "                patience=5,\n",
    "                verbose=False\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=30,  # Reduced for optimization\n",
    "            callbacks=callbacks,\n",
    "            logger=False,  # Disable logging for optimization\n",
    "            enable_progress_bar=False,\n",
    "            accelerator='auto',\n",
    "            devices=1\n",
    "        )\n",
    "        \n",
    "        # Train and get best validation metric\n",
    "        try:\n",
    "            trainer.fit(model, datamodule=self.data_module)\n",
    "            best_score = trainer.callback_metrics.get(\"val_win_precision\", float('-inf'))\n",
    "            if isinstance(best_score, torch.Tensor):\n",
    "                best_score = best_score.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed: {e}\")\n",
    "            best_score = float('-inf')\n",
    "            \n",
    "        return best_score\n",
    "\n",
    "def run_optuna_optimization(data_module: CryptoDataModule, n_trials: int = 50) -> Dict[str, Any]:\n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"crypto_price_prediction\",\n",
    "        pruner=optuna.pruners.MedianPruner()\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    objective = OptunaObjective(data_module)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Print optimization results\n",
    "    print(\"\\n=== Optimization Results ===\")\n",
    "    print(f\"Best trial value: {study.best_trial.value:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for param, value in study.best_trial.params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    return study.best_trial.params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory and data setup\n",
    "    directory_path = '/allah/data/parquet'\n",
    "    data_module = CryptoDataModule(directory_path=directory_path)\n",
    "    data_module.setup()\n",
    "    \n",
    "    # Run hyperparameter optimization\n",
    "    best_params = run_optuna_optimization(data_module, n_trials=50)\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    final_model = CryptoPricePredictor(\n",
    "        input_dim=data_module.input_dim,\n",
    "        hidden_dims=best_params['hidden_dims'],\n",
    "        dropout_rate=best_params['dropout_rate'],\n",
    "        learning_rate=best_params['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # Final training callbacks\n",
    "    final_callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_win_precision\",\n",
    "            mode=\"max\",\n",
    "            filename=\"best-model-{epoch:02d}-{val_win_precision:.2f}\",\n",
    "            save_top_k=3,\n",
    "            verbose=True\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_win_precision\",\n",
    "            mode=\"max\",\n",
    "            patience=10,\n",
    "            verbose=True\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ]\n",
    "    \n",
    "    # Final training\n",
    "    final_trainer = Trainer(\n",
    "        max_epochs=1000,\n",
    "        callbacks=final_callbacks,\n",
    "        logger=TensorBoardLogger(save_dir=directory_path, name=\"final_model_logs\"),\n",
    "        accelerator='auto',\n",
    "        devices=1\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    final_trainer.fit(final_model, datamodule=data_module)\n",
    "    print(f\"\\nFinal model best validation precision: {final_trainer.callback_metrics['val_win_precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cell\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "class TuneReportCallbackMetrics(TuneReportCallback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        metrics = {k: v.item() if hasattr(v, 'item') else v for k, v in metrics.items()}\n",
    "        self._report(metrics)\n",
    "\n",
    "def train_tune_model(config, data_module=None, num_epochs=30):\n",
    "    model = CryptoPricePredictor(\n",
    "        input_dim=data_module.input_dim,\n",
    "        hidden_dims=config[\"hidden_dims\"],\n",
    "        dropout_rate=config[\"dropout_rate\"],\n",
    "        learning_rate=config[\"learning_rate\"]\n",
    "    )\n",
    "    \n",
    "    data_module.batch_size = config[\"batch_size\"]\n",
    "    data_module.setup()\n",
    "    \n",
    "    callbacks = [\n",
    "        TuneReportCallbackMetrics(\n",
    "            metrics={\n",
    "                \"val_win_precision\": \"val_win_precision\",\n",
    "                \"val_loss\": \"val_loss\"\n",
    "            },\n",
    "            on=\"validation_end\"\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=3,\n",
    "            verbose=False\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        callbacks=callbacks,\n",
    "        logger=False,\n",
    "        enable_progress_bar=False,\n",
    "        accelerator='cpu',\n",
    "        devices=1,\n",
    "        gradient_clip_val=1.0,\n",
    "        accumulate_grad_batches=2\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "def run_ray_optimization(data_module, num_samples=20, num_epochs=20):\n",
    "    # Initialize Ray with updated configuration\n",
    "    ray.init(\n",
    "        ignore_reinit_error=True,\n",
    "        runtime_env={\n",
    "            \"env_vars\": {\n",
    "                \"RAY_memory_monitor_refresh_ms\": \"0\",\n",
    "                \"RAY_memory_usage_threshold\": \"0.95\"\n",
    "            }\n",
    "        },\n",
    "        _memory=2000 * 1024 * 1024\n",
    "    )\n",
    "    \n",
    "    config = {\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"hidden_dims\": tune.sample_from(lambda _: [\n",
    "            tune.randint(64, 256).sample() for _ in range(2)\n",
    "        ]),\n",
    "        \"dropout_rate\": tune.uniform(0.1, 0.5),\n",
    "        \"batch_size\": tune.choice([16, 32, 64])\n",
    "    }\n",
    "    \n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=num_epochs,\n",
    "        grace_period=3,\n",
    "        reduction_factor=3,\n",
    "        brackets=1\n",
    "    )\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"learning_rate\", \"dropout_rate\", \"batch_size\"],\n",
    "        metric_columns=[\"val_win_precision\", \"val_loss\", \"training_iteration\"]\n",
    "    )\n",
    "    \n",
    "    train_fn = partial(train_tune_model, data_module=data_module, num_epochs=num_epochs)\n",
    "    \n",
    "    # Updated tune.run with storage_path instead of local_dir\n",
    "    analysis = tune.run(\n",
    "        train_fn,\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"ray_tune_crypto\",\n",
    "        metric=\"val_win_precision\",\n",
    "        mode=\"max\",\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 1,\n",
    "            \"gpu\": 0\n",
    "        },\n",
    "        storage_path=\"/tmp/ray_results\",  # Changed from local_dir to storage_path\n",
    "        max_concurrent_trials=2,\n",
    "        raise_on_failed_trial=False\n",
    "    )\n",
    "    \n",
    "    best_trial = analysis.best_trial\n",
    "    best_config = best_trial.config\n",
    "    \n",
    "    print(\"\\n=== Ray Tune Optimization Results ===\")\n",
    "    print(f\"Best trial config: {best_config}\")\n",
    "    print(f\"Best trial final validation precision: {best_trial.last_result['val_win_precision']:.4f}\")\n",
    "    \n",
    "    ray.shutdown()\n",
    "    return best_config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = '/allah/data/parquet'\n",
    "    data_module = CryptoDataModule(directory_path=directory_path)\n",
    "    data_module.setup()\n",
    "    \n",
    "    try:\n",
    "        best_params = run_ray_optimization(data_module, num_samples=20, num_epochs=20)\n",
    "        \n",
    "        final_model = CryptoPricePredictor(\n",
    "            input_dim=data_module.input_dim,\n",
    "            hidden_dims=best_params['hidden_dims'],\n",
    "            dropout_rate=best_params['dropout_rate'],\n",
    "            learning_rate=best_params['learning_rate']\n",
    "        )\n",
    "        \n",
    "        final_callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                monitor=\"val_win_precision\",\n",
    "                mode=\"max\",\n",
    "                filename=\"best-model-{epoch:02d}-{val_win_precision:.2f}\",\n",
    "                save_top_k=1,\n",
    "                save_weights_only=True\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_win_precision\",\n",
    "                mode=\"max\",\n",
    "                patience=5,\n",
    "                verbose=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        final_trainer = Trainer(\n",
    "            max_epochs=100,\n",
    "            callbacks=final_callbacks,\n",
    "            logger=TensorBoardLogger(save_dir=directory_path, name=\"final_model_logs\"),\n",
    "            accelerator='cpu',\n",
    "            devices=1,\n",
    "            gradient_clip_val=1.0\n",
    "        )\n",
    "        \n",
    "        final_trainer.fit(final_model, datamodule=data_module)\n",
    "        print(f\"\\nFinal model best validation precision: {final_trainer.callback_metrics['val_win_precision']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during optimization: {e}\")\n",
    "    finally:\n",
    "        ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
