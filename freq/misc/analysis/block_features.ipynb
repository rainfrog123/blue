{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyarrow.feather as feather\n",
    "import pandas as pd\n",
    "import talib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the Block class\n",
    "class Block:\n",
    "    def __init__(self, start_date, end_date, start_price, end_price, duration, data_segment):\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.start_price = start_price\n",
    "        self.end_price = end_price\n",
    "        self.duration = duration\n",
    "        self.data_segment = data_segment  # Store all the data points in this block\n",
    "        self.direction = 'UP' if start_price < end_price else 'DOWN'\n",
    "        self.features = {}\n",
    "\n",
    "# Function to load cryptocurrency data\n",
    "def load_crypto_data(data_file_path, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Load cryptocurrency data, filter by date range, and calculate TEMA and trend information.\n",
    "    \n",
    "    Parameters:\n",
    "    data_file_path (str): Path to the Feather file containing the dataset.\n",
    "    start_date (str): The start date for filtering the data in 'YYYY-MM-DD' format.\n",
    "    end_date (str): The end date for filtering the data in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the filtered data with TEMA, trend, and trend change information.\n",
    "    \"\"\"\n",
    "    # Load data from Feather file\n",
    "    crypto_df = feather.read_feather(data_file_path)\n",
    "\n",
    "    # Convert input dates to datetime format\n",
    "    start_date = pd.to_datetime(start_date).tz_localize('UTC')\n",
    "    end_date = pd.to_datetime(end_date).tz_localize('UTC')\n",
    "    \n",
    "    # Filter data between the start and end dates\n",
    "    crypto_df = crypto_df[(crypto_df['date'] >= start_date) & (crypto_df['date'] <= end_date)]\n",
    "\n",
    "    # Calculate the Triple Exponential Moving Average (TEMA) with a default period of 50\n",
    "    tema_period = 50\n",
    "    crypto_df['tema'] = talib.TEMA(crypto_df['close'], timeperiod=tema_period)\n",
    "\n",
    "    # Determine the trend direction (UP, DOWN, STABLE)\n",
    "    crypto_df['trend'] = np.where(crypto_df['tema'] > crypto_df['tema'].shift(1), 'UP',\n",
    "                                  np.where(crypto_df['tema'] < crypto_df['tema'].shift(1), 'DOWN', 'STABLE'))\n",
    "\n",
    "    # Identify significant trend changes (ignoring 'STABLE' transitions)\n",
    "    crypto_df['is_trend_change'] = crypto_df['trend'] != crypto_df['trend'].shift(1)\n",
    "    crypto_df['is_significant_trend_change'] = crypto_df['is_trend_change'] & (crypto_df['trend'] != 'STABLE')\n",
    "\n",
    "    # Assign a unique group ID to each continuous trend segment\n",
    "    crypto_df['group_id'] = crypto_df['is_significant_trend_change'].cumsum()\n",
    "\n",
    "    return crypto_df\n",
    "\n",
    "# Function to create blocks from the data\n",
    "def create_blocks(crypto_df):\n",
    "    \"\"\"\n",
    "    Create a list of Block objects from the DataFrame.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "\n",
    "    # Group the data by 'group_id'\n",
    "    grouped = crypto_df.groupby('group_id')\n",
    "\n",
    "    # Iterate over each group and create a block\n",
    "    for group_id, group_data in grouped:\n",
    "        start_date = group_data['date'].iloc[0]\n",
    "        end_date = group_data['date'].iloc[-1]\n",
    "        start_price = group_data['close'].iloc[0]\n",
    "        end_price = group_data['close'].iloc[-1]\n",
    "        duration = len(group_data)\n",
    "\n",
    "        # Create a block object that includes all the data points\n",
    "        block = Block(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            start_price=start_price,\n",
    "            end_price=end_price,\n",
    "            duration=duration,\n",
    "            data_segment=group_data  # Store the entire segment of data\n",
    "        )\n",
    "\n",
    "        # Add the block to the list\n",
    "        blocks.append(block)\n",
    "\n",
    "    return blocks\n",
    "\n",
    "# Example usage (for debugging purposes):\n",
    "data_file_path = '/allah/freqtrade/user_data/data/binance/futures/ETH_USDT_USDT-3m-futures.feather'\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2024-10-22'\n",
    "\n",
    "# Load the data\n",
    "crypto_df = load_crypto_data(data_file_path, start_date, end_date)\n",
    "# calculated RSI here\n",
    "crypto_df['rsi'] = talib.RSI(crypto_df['close'], timeperiod=14)\n",
    "# Create blocks\n",
    "blocks = create_blocks(crypto_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_statistical_features(series):\n",
    "    return {\n",
    "        'mean': np.mean(series),\n",
    "        'median': np.median(series),\n",
    "        'std': np.std(series),\n",
    "        'variance': np.var(series),\n",
    "        'skewness': series.skew(),\n",
    "        'kurtosis': series.kurtosis(),\n",
    "        'min': np.min(series),\n",
    "        'max': np.max(series)\n",
    "    }\n",
    "\n",
    "def extract_ts_features(block):\n",
    "    features = {'length': block.duration}\n",
    "    rsi_series = block.data_segment['rsi'].dropna()  # Directly use RSI from data segment\n",
    "\n",
    "    if not rsi_series.empty:\n",
    "        # Calculate statistical features for RSI\n",
    "        rsi_stat_features = calculate_statistical_features(rsi_series)\n",
    "\n",
    "        # Update features with RSI statistical metrics\n",
    "        features.update({f'rsi_{k}': v for k, v in rsi_stat_features.items()})\n",
    "\n",
    "        # Add RSI level counts\n",
    "        features['rsi_above_70'] = np.sum(rsi_series > 70)\n",
    "        features['rsi_below_30'] = np.sum(rsi_series < 30)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Iterate over blocks and extract RSI features\n",
    "from tqdm import tqdm\n",
    "\n",
    "for block in tqdm(blocks, desc=\"Extracting RSI Features\"):\n",
    "    block.features = extract_ts_features(block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks[-24].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import mplfinance as mpf\n",
    "\n",
    "# # Prepare the data in the correct format\n",
    "# data_segment = blocks[-1].data_segment  # Assume this is the OHLCV data\n",
    "\n",
    "# data_segment['date'] = pd.to_datetime(data_segment['date'])\n",
    "\n",
    "# # Set the 'date' as the index\n",
    "# data_segment.set_index('date', inplace=True)\n",
    "\n",
    "# # Rename the columns to match mplfinance requirements\n",
    "# data_segment.rename(columns={\n",
    "#     'open': 'Open', \n",
    "#     'high': 'High', \n",
    "#     'low': 'Low', \n",
    "#     'close': 'Close', \n",
    "#     'volume': 'Volume'\n",
    "# }, inplace=True)\n",
    "\n",
    "# # Plot the OHLCV data as candlesticks\n",
    "# mpf.plot(\n",
    "#     data_segment, \n",
    "#     type='candle', \n",
    "#     volume=True,  # Include volume subplot\n",
    "#     style='charles',  # Choose a style (optional)\n",
    "#     title=f'Candlestick Chart from {data_segment.index.min().date()} to {data_segment.index.max().date()}',\n",
    "#     ylabel='Price (USD)',\n",
    "#     ylabel_lower='Volume',\n",
    "#     figsize=(14, 8)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Function to replace NaN with the last valid non-NaN value from previous blocks' features\n",
    "def replace_nan_in_features_with_last_valid(blocks):\n",
    "    last_valid_values = {}\n",
    "\n",
    "    for block in blocks:\n",
    "        filled_features = {}\n",
    "        \n",
    "        # Iterate only over features within the block\n",
    "        for key, value in block.features.items():\n",
    "            # If value is NaN, use the last valid non-NaN value from previous blocks\n",
    "            if isinstance(value, float) and math.isnan(value):\n",
    "                # If we have a previous valid value, use it\n",
    "                value = last_valid_values.get(key, 0)  # Default to 0 if no valid value found\n",
    "            else:\n",
    "                # Update last valid value for this key\n",
    "                last_valid_values[key] = value\n",
    "            filled_features[key] = value\n",
    "\n",
    "        # Update the block's features with the filled values\n",
    "        block.features = filled_features\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "# Apply the function to replace NaNs in block features\n",
    "blocks = replace_nan_in_features_with_last_valid(blocks)\n",
    "\n",
    "# Display the result to verify NaNs are replaced in features only\n",
    "for i, block in enumerate(blocks[:5]):  # Displaying only the first 5 blocks as an example\n",
    "    print(f\"Block {i} features:\", block.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Y = []\n",
    "\n",
    "for i in range(len(blocks)):\n",
    "    # Extract the last timestamp in the current block\n",
    "    timestamp = blocks[i].data_segment.iloc[-1].date\n",
    "    \n",
    "    # Calculate the timestamp for 10 candles (30 minutes later)\n",
    "    timestamp_after_10 = timestamp + pd.Timedelta(minutes=30)\n",
    "    \n",
    "    # Extract the corresponding close price from crypto_df for the timestamp after 10 candles\n",
    "    close_price_after_10 = crypto_df.loc[crypto_df['date'] == timestamp_after_10, 'close']\n",
    "    \n",
    "    # Get the end price for the current block's timestamp\n",
    "    end_price = crypto_df.loc[crypto_df['date'] == timestamp, 'close']\n",
    "    \n",
    "    # Check if end_price and close_price_after_10 are non-empty before accessing values\n",
    "    if not end_price.empty and not close_price_after_10.empty:\n",
    "        blocks[i].end_price = end_price.iloc[0]\n",
    "        close_price_after_10_value = close_price_after_10.iloc[0]\n",
    "        \n",
    "        # Compare prices and append to Y\n",
    "        if close_price_after_10_value > blocks[i].end_price:\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "    else:\n",
    "        # Append None if no matching timestamp is found in crypto_df\n",
    "        Y.append(None)\n",
    "\n",
    "# Now Y contains 1 or 0 based on price movement, or None if data is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the length of each sequence\n",
    "k = 10\n",
    "\n",
    "# Generate X: sequences of features from the previous k blocks in correct order\n",
    "X = [[blocks[i + j].features for j in range(k)] for i in range(len(blocks) - k + 1)]\n",
    "\n",
    "# Align Y with X by starting Y from the corresponding index\n",
    "Y_aligned = Y[k - 1:]\n",
    "\n",
    "# Print lengths to verify alignment\n",
    "print(\"Length of X:\", len(X))\n",
    "print(\"Length of Y_aligned:\", len(Y_aligned))\n",
    "\n",
    "X_test = X[:-1]\n",
    "Y_test = Y_aligned[:-1]\n",
    "\n",
    "print(\"Length of X_test:\", len(X_test))\n",
    "print(\"Length of Y_test:\", len(Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Function to convert a dictionary to a feature vector\n",
    "def dict_to_feature_vector(dictionary):\n",
    "    return [\n",
    "        0 if isinstance(value, float) and math.isnan(value) else value \n",
    "        for value in dictionary.values()\n",
    "    ]\n",
    "\n",
    "# Convert each sequence (list of dictionaries) in X_test into a 2D array\n",
    "X_numeric = [\n",
    "    [dict_to_feature_vector(block) for block in sequence] \n",
    "    for sequence in X_test\n",
    "]\n",
    "\n",
    "# Convert X_numeric to a numpy array for LSTM input\n",
    "X_numeric = np.array(X_numeric)\n",
    "\n",
    "# Reshape for LSTM input: (samples, time steps, features)\n",
    "# Here, each sequence is treated as a sample with multiple time steps\n",
    "print(\"Shape of X_numeric:\", X_numeric.shape)  # Expected shape: (number of samples, time steps, number of features)\n",
    "\n",
    "Y_numeric = np.array(Y_test)\n",
    "\n",
    "# Print the shape of Y_numeric\n",
    "print(\"Shape of Y_numeric:\", Y_numeric.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq = np.unique(Y_numeric, return_counts=True)\n",
    "print(\"Unique values in Y_numeric:\", dict(zip(*uniq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X_numeric and Y_numeric are already defined\n",
    "# Example shape of X_numeric: (samples, time steps, features) -> (1321, 10, 11)\n",
    "\n",
    "# Reshape X_numeric to 2D for scaling\n",
    "X_reshaped = X_numeric.reshape(-1, X_numeric.shape[2])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "\n",
    "# Reshape back to original shape for LSTM input\n",
    "X_scaled = X_scaled.reshape(X_numeric.shape[0], X_numeric.shape[1], X_numeric.shape[2])\n",
    "\n",
    "print(\"Shape of X:\", X_scaled.shape)  # Expected shape: (samples, time steps, features)\n",
    "print(\"Shape of Y:\", Y_numeric.shape)  # Shape of output labels\n",
    "\n",
    "# Build the LSTM model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add LSTM layers\n",
    "model.add(layers.LSTM(units=50, input_shape=(X_scaled.shape[1], X_scaled.shape[2]), return_sequences=False))\n",
    "\n",
    "# Add dropout layer\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add output layer for binary classification\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# TensorBoard callback for logging training metrics\n",
    "log_dir = \"/allah/data/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_scaled, Y_numeric, epochs=200, batch_size=32, validation_split=0.2, callbacks=[tensorboard_callback])\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN and Inf values in X_scaled\n",
    "print(\"NaN values in X_scaled:\", np.isnan(X_scaled).sum())  # Should be 0\n",
    "print(\"Inf values in X_scaled:\", np.isinf(X_scaled).sum())  # Should be 0\n",
    "# Check for NaN and Inf values in Y\n",
    "print(\"NaN values in Y:\", np.isnan(Y).sum())  # Should be 0\n",
    "print(\"Inf values in Y:\", np.isinf(Y).sum())  # Should be 0\n",
    "\n",
    "# Ensure Y is strictly binary (0 or 1)\n",
    "print(\"Unique values in Y:\", np.unique(Y))  # Should only be [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
